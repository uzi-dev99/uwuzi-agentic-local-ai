Plan de Acci√≥n v2: De Prototipo a APK con Backend Inteligente Multimodal
Este documento es una lista de tareas secuencial para transformar la aplicaci√≥n Wuzi. El objetivo es evolucionar de un prototipo conectado a una API externa a una aplicaci√≥n robusta con un backend propio que act√∫a como agente enrutador multimodal usando Gemma 3n, y finalmente, empaquetarla como una aplicaci√≥n nativa para Android.
Modelo Base: Gemma 3n (google/gemma-3n-2b-it / google/gemma-3n-4b-it)
Especificaciones T√©cnicas:
* üî• Contexto de 32,000 tokens: Capacidad de procesamiento de contexto sustancial.
* üöÄ Arquitectura MatFormer: Modelos anidados para optimizaci√≥n de recursos.
* üíæ Cach√© PLE: Par√°metros de incorporaci√≥n por capa almacenables en cach√©.
* ‚ö° Carga Condicional: Omisi√≥n de par√°metros de audio/visual para ahorrar memoria.
* üåç 140+ idiomas: Soporte multiling√ºe extenso.
* üì± Optimizado para dispositivos: Dise√±ado para tel√©fonos, laptops y tablets.
Capacidades Multimodales:
* ‚úÖ Procesamiento de texto: An√°lisis y generaci√≥n de texto avanzado.
* ‚úÖ Entrada visual: Codificador MobileNet-V5 para procesamiento de im√°genes.
* ‚úÖ Entrada de audio: Reconocimiento de voz, traducci√≥n y an√°lisis de audio.
* ‚úÖ Conversaciones multi-turno: Mantenimiento de contexto en conversaciones largas.
* ‚úÖ Seguimiento de instrucciones complejas: Comprensi√≥n contextual avanzada.
Optimizaciones de Par√°metros:
* Modelo E2B: ~5B par√°metros totales, ~1.9B par√°metros efectivos con optimizaciones.
* Modelo E4B: Contiene par√°metros del E2B + adicionales, arquitectura anidada.
* T√©cnicas de eficiencia: PLE cach√©, MatFormer, carga condicional de par√°metros.
Fase 1: Refactorizaci√≥n del Backend (Agente Enrutador)
Objetivo: Convertir el backend de FastAPI en un "agente" inteligente que reciba todas las peticiones, determine la intenci√≥n del usuario y las enrute al servicio apropiado (Ollama para chat, n8n para workflows).
1.1. Preparaci√≥n y Estructura del Proyecto
* [x] Verificar Estructura de Carpetas: Aseg√∫rate de que la estructura del proyecto backend sea la siguiente:
/backend
|-- main.py
|-- requirements.txt
|-- .env
|-- /services
|   |-- __init__.py
|   |-- agent_service.py
|   |-- chat_service.py
|   |-- n8n_service.py
|-- /models
   |-- __init__.py
   |-- agent_models.py

* [x] Actualizar Dependencias (requirements.txt): Aseg√∫rate de que el archivo contenga las librer√≠as necesarias.
fastapi
uvicorn
httpx
python-dotenv

* [x] Configurar Variables de Entorno (.env): Crea o actualiza el archivo .env en la ra√≠z de /backend.
OLLAMA_API_URL="http://localhost:11434/api/chat"
N8N_SALES_REPORT_WEBHOOK="TU_WEBHOOK_DE_N8N_AQUI"
AGENT_ROUTER_MODEL="gemma3n:e4b"
# Nota: Modelo Ollama local con arquitectura E4B (par√°metros anidados optimizados)

# Configuraci√≥n espec√≠fica para Gemma 3n
GEMMA_CONTEXT_LENGTH=32000
GEMMA_ENABLE_PLE_CACHE=true
GEMMA_CONDITIONAL_LOADING=true
GEMMA_MATFORMER_MODE=true

1.2. Implementaci√≥n de Servicios
   * [x] Implementar services/chat_service.py: Este servicio se comunicar√° con Ollama para el chat conversacional.
   * Crea una funci√≥n as√≠ncrona stream_chat_with_ollama(messages: list) que reciba el historial de mensajes.
   * Dentro de la funci√≥n, usa httpx.AsyncClient para hacer una petici√≥n POST a la OLLAMA_API_URL.
   * Aseg√∫rate de que el payload incluya {"stream": True}.
   * La funci√≥n debe ser un generador (async for ... yield) que devuelva los chunks de datos tal como los recibe de Ollama.
   * [x] Implementar services/n8n_service.py (Placeholder): Este servicio activar√° los workflows en n8n.
   * Crea una funci√≥n as√≠ncrona trigger_sales_report(params: dict).
   * Dentro de la funci√≥n, usa httpx.AsyncClient para hacer una petici√≥n POST a la N8N_SALES_REPORT_WEBHOOK.
   * La funci√≥n debe devolver un diccionario con el estado de la operaci√≥n (ej: {"status": "success", "message": "Reporte de ventas en proceso."}).
   * [x] Implementar el Agente (services/agent_service.py): Este es el cerebro del backend.
   * classify_intent(user_prompt: str):
   * Crea una funci√≥n as√≠ncrona para determinar la intenci√≥n.
   * Define un system_prompt claro que instruya al AGENT_ROUTER_MODEL para que clasifique el user_prompt y devuelva una categor√≠a espec√≠fica (ej: conversational_chat, sales_report_workflow).
   * Realiza una petici√≥n POST (no streaming) a Ollama con el system_prompt y el user_prompt.
   * Extrae y devuelve la categor√≠a de la respuesta del modelo.
   * invoke_agent(request_data: dict):
   * Crea la funci√≥n generadora as√≠ncrona principal.
   * Extrae el √∫ltimo mensaje del usuario del request_data.
   * Llama a classify_intent para obtener la intenci√≥n.
   * Usa una estructura if/elif/else basada en la intent:
   * Si la intenci√≥n es sales_report_workflow, llama a n8n_service.trigger_sales_report y usa yield para devolver su resultado como un √∫nico chunk.
   * Si no, (por defecto conversational_chat), itera sobre el generador chat_service.stream_chat_with_ollama y usa yield para pasar cada chunk al cliente.
1.3 Exponer el Endpoint Principal
   * [x] Actualizar main.py:
   * Elimina cualquier endpoint antiguo (como /direct_chat).
   * Importa el agent_service.
   * Configura CORSMiddleware para permitir peticiones desde el frontend durante el desarrollo (http://localhost:5173).
   * Crea un √∫nico endpoint POST en la ruta /api/v1/agent/invoke.
   * Dentro de este endpoint, llama a agent_service.invoke_agent con el cuerpo de la petici√≥n.
   * Envuelve la llamada en un StreamingResponse con media_type="application/x-ndjson".
1.4. Pruebas del Backend
   * [x] Prueba de Endpoint: Una vez que el backend est√© corriendo, usa una herramienta como curl para probar el endpoint y verificar que el enrutamiento funciona.
# Prueba de chat conversacional
curl -X POST http://localhost:8000/api/v1/agent/invoke -H "Content-Type: application/json" -d '{"messages": [{"role": "user", "content": "Hola, ¬øc√≥mo est√°s?"}]}'

# Prueba de workflow (simulada)
curl -X POST http://localhost:8000/api/v1/agent/invoke -H "Content-Type: application/json" -d '{"messages": [{"role": "user", "content": "Genera el reporte de ventas"}]}'

Fase 2: Adaptaci√≥n del Frontend (React)
Objetivo: Desconectar el frontend de la API de Gemini y conectarlo exclusivamente al nuevo endpoint del agente en nuestro backend.
      * [x] Configurar Variables de Entorno (frontend/.env.local):
      * Elimina cualquier clave de API de Gemini.
      * A√±ade la URL del backend. Para producci√≥n, ser√° la URL p√∫blica.
VITE_BACKEND_API_URL=https://backend-nodo-uno.wuzi.uk/api/v1

         * [x] Crear Nuevo Servicio de API (src/services/backendService.ts):
         * Crea un nuevo archivo para la comunicaci√≥n con tu backend.
         * Define una funci√≥n as√≠ncrona invokeAgent(messages: Message[], onChunk: (chunk: string) => void, onError: (error: Error) => void).
         * Dentro de la funci√≥n, usa fetch para hacer una petici√≥n POST a ${VITE_BACKEND_API_URL}/agent/invoke.
         * Adapta el array de messages al formato que espera el backend (ej: {role: msg.role, content: msg.text}).
         * Implementa la l√≥gica para leer la respuesta como un stream (response.body.getReader()).
         * Decodifica cada value del stream y procesa los chunks de JSON delimitados por saltos de l√≠nea (\n).
         * Por cada chunk de contenido v√°lido, llama al callback onChunk(contenido).
         * [x] Integrar el Nuevo Servicio en la L√≥gica del Chat:
         * Ve al archivo que gestiona el env√≠o de mensajes (probablemente src/contexts/ChatContext.tsx).
         * Importa la nueva funci√≥n invokeAgent de backendService.ts.
         * En la funci√≥n que env√≠a el mensaje (handleSendMessage o similar), reemplaza la llamada a la API de Gemini por una llamada a invokeAgent.
         * Pasa las funciones de callback para onChunk (que actualiza el mensaje del asistente en el estado de React) y onError (que muestra un mensaje de error).
         * [x] Limpieza del Proyecto:
         * Elimina el archivo src/services/geminiService.ts.
         * Busca y elimina cualquier referencia restante a Gemini o su API key en todo el proyecto.
         * [x] Pruebas Full-Stack:
         * Ejecuta el backend y el frontend simult√°neamente.
         * Abre la aplicaci√≥n web en tu navegador.
         * Prueba ambas funcionalidades: el chat conversacional y la activaci√≥n de un workflow. Verifica que la UI se actualice correctamente en ambos casos.
Fase 2.5: Establecimiento de Chat Estable con Ollama
Objetivo: Garantizar que el frontend pueda mantener conversaciones fluidas y estables con modelos de Ollama a trav√©s del backend HTTPS, espec√≠ficamente en modo chat conversacional.
2.5.1. Verificaci√≥n de Conectividad con Ollama
         * [ ] Verificar Ollama Local: Confirmar que Ollama est√° ejecut√°ndose localmente y responde correctamente.
         * [ ] Probar Conexi√≥n Directa: Realizar pruebas directas desde el backend hacia Ollama para verificar la comunicaci√≥n.
         * [ ] Validar Modelos Disponibles: Confirmar que el modelo configurado (llama3) est√° disponible y funcional.
2.5.2. Estrategia de Gesti√≥n de Contexto y Memoria
         * [ ] An√°lisis de Limitaciones de Contexto:
         * Determinar el l√≠mite de tokens del modelo Ollama configurado.
         * Calcular cu√°ntos mensajes promedio caben en el contexto.
         * Establecer umbrales de memoria (ej: 80% del l√≠mite).
         * [ ] Implementaci√≥n de Gesti√≥n de Contexto:
         * Estrategia de Ventana Deslizante: Mantener los N mensajes m√°s recientes.
         * Preservaci√≥n de Contexto Inicial: Siempre mantener el primer mensaje del chat.
         * Resumen Inteligente: Generar res√∫menes de conversaciones largas.
         * Compresi√≥n de Mensajes: Optimizar mensajes largos sin perder informaci√≥n clave.
         * [ ] Configuraci√≥n de Par√°metros:
         * MAX_CONTEXT_MESSAGES: N√∫mero m√°ximo de mensajes en contexto (ej: 20).
         * CONTEXT_WINDOW_SIZE: Tama√±o de ventana deslizante (ej: 15 mensajes).
         * SUMMARY_THRESHOLD: Umbral para activar resumen (ej: 25 mensajes).
         * PRESERVE_SYSTEM_MESSAGES: Mantener mensajes del sistema siempre.
         * [ ] Implementaci√≥n en Backend:
         * Crear funci√≥n manage_context() en chat_service.py.
         * Implementar l√≥gica de truncamiento inteligente.
         * A√±adir generaci√≥n de res√∫menes autom√°ticos.
         * Optimizar el payload enviado a Ollama.
         * [ ] Implementaci√≥n en Frontend:
         * Mostrar indicador de contexto limitado al usuario.
         * Opci√≥n para ver historial completo vs contexto activo.
         * Notificaci√≥n cuando se activa el resumen autom√°tico.
2.5.3. Depuraci√≥n del Servicio de Chat
         * [ ] Revisar chat_service.py: Analizar y corregir cualquier problema en la comunicaci√≥n con Ollama.
         * [ ] Implementar Logging Detallado: A√±adir logs espec√≠ficos para rastrear el flujo de datos entre frontend, backend y Ollama.
         * [ ] Manejar Errores de Conexi√≥n: Implementar manejo robusto de errores cuando Ollama no est√© disponible.
2.5.4. Optimizaci√≥n del Frontend para Chat
         * [ ] Verificar Streaming de Respuestas: Asegurar que el frontend procesa correctamente las respuestas en streaming de Ollama.
         * [ ] Implementar Indicadores de Estado: A√±adir indicadores visuales de "escribiendo" y manejo de errores de conexi√≥n.
         * [ ] Probar Conversaciones Largas: Verificar que el historial de mensajes se mantiene correctamente en conversaciones extensas.
2.5.5. Pruebas de Estabilidad
         * [ ] Chat Continuo: Realizar m√∫ltiples intercambios de mensajes para verificar estabilidad.
         * [ ] Manejo de Reconexi√≥n: Probar el comportamiento cuando se pierde y recupera la conexi√≥n.
         * [ ] Rendimiento: Verificar que las respuestas llegan en tiempo razonable y sin interrupciones.
2.5.6. Validaci√≥n Final del Chat
         * [ ] Conversaci√≥n Completa: Mantener una conversaci√≥n de al menos 10 intercambios exitosos.
         * [ ] Diferentes Tipos de Consultas: Probar preguntas cortas, largas, t√©cnicas y conversacionales.
         * [ ] Estabilidad Prolongada: Verificar que el chat funciona de manera estable durante al menos 15 minutos de uso continuo.
Fase 2.6: Implementaci√≥n de Capacidades Multimodales con Gemma 3n
Objetivo: Aprovechar las capacidades multimodales avanzadas de Gemma 3n para procesar im√°genes y audio adem√°s de texto, utilizando sus t√©cnicas de optimizaci√≥n de par√°metros.
2.6.1. Configuraci√≥n del Modelo Multimodal
         * [ ] Actualizar Configuraci√≥n del Backend:
         * Cambiar AGENT_ROUTER_MODEL a "google/gemma-3n-2b-it" o "google/gemma-3n-4b-it" en .env.
         * Verificar que Ollama tiene el modelo Gemma 3n disponible.
         * Configurar par√°metros espec√≠ficos para procesamiento multimodal.
         * Habilitar contexto extendido de 32,000 tokens.
         * Configurar t√©cnicas de optimizaci√≥n (PLE cach√©, MatFormer, carga condicional).
         * [ ] Adaptar chat_service.py para Multimodalidad:
         * Modificar stream_chat_with_ollama para soportar mensajes con im√°genes.
         * Implementar manejo de diferentes tipos de contenido (text, image, audio).
         * A√±adir validaci√≥n de formatos de archivo soportados.
2.6.2. Soporte de Im√°genes
         * [ ] Backend - Procesamiento de Im√°genes:
         * Implementar endpoint para subida de im√°genes.
         * A√±adir validaci√≥n de tipos de archivo (jpg, png, webp).
         * Configurar l√≠mites de tama√±o de archivo.
         * Implementar conversi√≥n a base64 para env√≠o a Ollama.
         * [ ] Frontend - Interfaz de Im√°genes:
         * Actualizar MessageInput.tsx para soportar selecci√≥n de im√°genes.
         * A√±adir preview de im√°genes antes del env√≠o.
         * Implementar drag & drop para im√°genes.
         * Mostrar im√°genes en el historial de mensajes.
         * [ ] Integraci√≥n con Gemma 3n:
         * Adaptar formato de mensajes para incluir im√°genes con codificador MobileNet-V5.
         * Usar tokens especiales <image_soft_token> seg√∫n documentaci√≥n.
         * Implementar plantillas de instrucci√≥n multimodales.
         * Aprovechar el contexto extendido de 32,000 tokens para im√°genes complejas.
         * Configurar carga condicional de par√°metros visuales.
2.6.3. Soporte de Audio
         * [ ] Backend - Procesamiento de Audio:
         * Implementar endpoint para subida de archivos de audio.
         * A√±adir validaci√≥n de formatos (wav, mp3, m4a).
         * Configurar l√≠mites de duraci√≥n y tama√±o.
         * Implementar conversi√≥n de formatos si es necesario.
         * [ ] Frontend - Interfaz de Audio:
         * Integrar grabaci√≥n de audio en tiempo real.
         * A√±adir controles de reproducci√≥n para audio.
         * Implementar visualizaci√≥n de ondas de audio.
         * Mostrar archivos de audio en el historial.
         * [ ] Integraci√≥n con Gemma 3n:
         * Usar tokens <audio_soft_token> para audio.
         * Implementar plantillas de instrucci√≥n para audio.
         * Configurar procesamiento de transcripci√≥n y an√°lisis.
         * Aprovechar soporte nativo para reconocimiento de voz y traducci√≥n.
         * Utilizar contexto de 32,000 tokens para an√°lisis de audio largo.
         * Configurar carga condicional de par√°metros de audio.
2.6.4. Optimizaci√≥n Multimodal
         * [ ] Gesti√≥n de Contexto Multimodal Extendido:
         * Adaptar estrategia de gesti√≥n de contexto para aprovechar 32,000 tokens.
         * Implementar compresi√≥n inteligente de im√°genes en contexto.
         * Optimizar almacenamiento temporal de archivos multimedia.
         * Configurar ventana deslizante inteligente para contenido multimodal.
         * [ ] T√©cnicas de Optimizaci√≥n Gemma 3n:
         * Implementar cach√© PLE para par√°metros de incorporaci√≥n por capa.
         * Configurar arquitectura MatFormer para modelos anidados.
         * Habilitar carga condicional de par√°metros (audio/visual).
         * Optimizar uso de memoria con par√°metros efectivos.
         * Configurar almacenamiento en cach√© local r√°pido para PLE.
         * [ ] Rendimiento y Cach√© Avanzado:
         * Implementar cach√© para im√°genes procesadas con MobileNet-V5.
         * Optimizar tiempo de respuesta para contenido multimodal.
         * Configurar l√≠mites de memoria din√°micos seg√∫n disponibilidad.
         * Implementar selecci√≥n autom√°tica entre E2B y E4B seg√∫n recursos.
2.6.5. Pruebas Multimodales
         * [ ] Pruebas de Im√°genes:
         * Enviar im√°genes simples con preguntas descriptivas.
         * Probar an√°lisis de contenido visual.
         * Verificar manejo de m√∫ltiples im√°genes en una conversaci√≥n.
         * [ ] Pruebas de Audio:
         * Enviar archivos de audio para transcripci√≥n.
         * Probar an√°lisis de contenido de audio.
         * Verificar calidad de transcripci√≥n en diferentes idiomas.
         * [ ] Pruebas Combinadas:
         * Conversaciones con texto, im√°genes y audio mezclados.
         * Verificar coherencia en respuestas multimodales.
         * Probar l√≠mites de procesamiento simult√°neo.
Fase 3: Empaquetado para Android (Creaci√≥n del APK)
Objetivo: Envolver la aplicaci√≥n web de React en un contenedor nativo para crear un archivo .apk instalable en dispositivos Android. Usaremos Capacitor, el sucesor moderno de Cordova.
3.1. Prerrequisitos
         * [ ] Instalar Android Studio: Desc√°rgalo e inst√°lalo desde el sitio oficial de Android.
         * [ ] Instalar un JDK (Java Development Kit): Android Studio suele venir con uno, pero puedes instalarlo por separado si es necesario (se recomienda OpenJDK).
3.2. Preparar el Proyecto de React para Capacitor
         * [ ] Instalar Dependencias de Capacitor: En la terminal, dentro de la carpeta /frontend, ejecuta:
npm install @capacitor/core @capacitor/cli @capacitor/android

         * [ ] Inicializar Capacitor: Este comando crea el archivo de configuraci√≥n de Capacitor.
npx cap init "Wuzi Assist" "uk.wuzi.assist" --web-dir "dist"

            * "Wuzi Assist": El nombre de tu aplicaci√≥n que aparecer√° en el tel√©fono.
            * "uk.wuzi.assist": El ID √∫nico del paquete (formato de dominio inverso).
            * --web-dir "dist": Le dice a Capacitor que los archivos de tu web de producci√≥n estar√°n en la carpeta /dist.
3.3. Construcci√≥n y Sincronizaci√≥n
            * [ ] Construir la Aplicaci√≥n de React: Crea la versi√≥n de producci√≥n de tu app.
npm run build

            * [ ] A√±adir la Plataforma Android: Este comando crea una carpeta /android dentro de tu proyecto de frontend que contiene un proyecto nativo de Android.
npx cap add android

            * [ ] Sincronizar los Archivos Web: Copia los √∫ltimos cambios de tu carpeta /dist al proyecto nativo de Android. Debes ejecutar este comando cada vez que hagas cambios en tu c√≥digo de React.
npx cap sync

3.4. Configuraci√≥n en Android Studio
               * [ ] Abrir el Proyecto Nativo:
npx cap open android

Esto abrir√° Android Studio con tu proyecto.
               * [ ] Configurar Permisos de Red:
                  * En Android Studio, busca el archivo android/app/src/main/AndroidManifest.xml.
                  * Aseg√∫rate de que el permiso de internet est√© presente justo antes de la etiqueta <application>:
<uses-permission android:name="android.permission.INTERNET" />

                     * [ ] Verificar la URL del Servidor (Opcional, pero recomendado):
                     * En el archivo capacitor.config.ts de tu proyecto de React, puedes configurar la URL del servidor para que la app sepa a d√≥nde apuntar.
// capacitor.config.ts
import { CapacitorConfig } from '@capacitor/cli';

const config: CapacitorConfig = {
 // ...
 server: {
   hostname: 'backend-nodo-uno.wuzi.uk',
   androidScheme: 'https'
 }
};

export default config;

                     * Despu√©s de cambiar esto, vuelve a ejecutar npx cap sync.
3.5. Generar el APK Firmado
                        * [ ] Crear una Clave de Firma (Keystore):
                        * En Android Studio, ve a Build > Generate Signed Bundle / APK....
                        * Selecciona APK y haz clic en Next.
                        * Haz clic en Create new... debajo del campo "Key store path".
                        * Rellena el formulario para crear tu archivo de clave (.jks). Guarda este archivo y sus contrase√±as en un lugar seguro. Lo necesitar√°s para todas las futuras actualizaciones de tu app.
                        * Haz clic en OK.
                        * [ ] Generar el APK:
                        * Con los datos de tu keystore ya cargados, selecciona el tipo de build release.
                        * Haz clic en Create.
                        * Una vez que termine el proceso, Android Studio mostrar√° una notificaci√≥n con un enlace para locate (localizar) tu APK. El archivo estar√° t√≠picamente en frontend/android/app/release/app-release.apk.
                        * [ ] ¬°Listo! Ya tienes tu archivo .apk que puedes instalar en un dispositivo Android para probar.